From 5e4d3d26da6520f8bbcb4cb8f9706c3bf55eece6 Mon Sep 17 00:00:00 2001
From: Ping Gao <ping.a.gao@intel.com>
Date: Tue, 6 Sep 2016 21:13:10 +0800
Subject: [PATCH 428/433] vgt: qos: factor out the scheduler

Factor out the scheduler, the new function vgt_pickup_next() will
determine which guest to be schedule next.
The longest unsched guest will get the highest priority to schedule
immediately to avoid TDR if it have not been served over a threshold
time.

Signed-off-by: Ping Gao <ping.a.gao@intel.com>
Reviewed-by: Kevin Tian <kevin.tian@intel.com>
---
 drivers/gpu/drm/i915/vgt/sched.c |   78 +++++++++++++++++++++++++++++---------
 1 file changed, 61 insertions(+), 17 deletions(-)

diff --git a/drivers/gpu/drm/i915/vgt/sched.c b/drivers/gpu/drm/i915/vgt/sched.c
index d95b42c..5502709 100644
--- a/drivers/gpu/drm/i915/vgt/sched.c
+++ b/drivers/gpu/drm/i915/vgt/sched.c
@@ -67,23 +67,12 @@ static inline bool phys_head_catch_tail(struct pgt_device *pdev)
 	return true;
 }
 
-/* FIXME: Since it is part of "timer based scheduler",
- * move this from vgt_context.c here and renamed from
- * next_vgt() to tbs_next_vgt()
- */
-static struct vgt_device *tbs_next_vgt(
-	struct list_head *head, struct vgt_device *vgt)
+static struct vgt_device *vgt_pickup_next(struct list_head *head,
+	struct vgt_device *cur_vgt)
 {
-	struct list_head *next = &vgt->list;
 	struct vgt_device *next_vgt = NULL;
-	struct pgt_device *pdev;
-
-	if (vgt->force_removal)
-		return vgt_dom0;
-
-	pdev = vgt->pdev;
-	if (ctx_switch_requested(pdev))
-		return pdev->next_sched_vgt;
+	struct pgt_device *pdev = cur_vgt->pdev;
+	struct list_head *next = &cur_vgt->list;
 
 	do {
 		next = next->next;
@@ -96,12 +85,67 @@ static struct vgt_device *tbs_next_vgt(
 			pdev->dummy_vm_switch = false;
 			break;
 		}
-
-	} while (next_vgt != vgt);
+	} while (next_vgt != cur_vgt);
 
 	return next_vgt;
 }
 
+/* Find out the guest which have not been scheduled near a max
+ * threshold, then execute it immediately to avoid guest TDR.
+ * TODO: change to more efficient sorted list.
+ */
+#define GUEST_TDR_THRES_MS 1500
+static struct vgt_device *vgt_longest_unsched(struct list_head *head)
+{
+	struct vgt_device *vgt = NULL;
+	cycles_t cur_cycles = vgt_get_cycles();
+
+	list_for_each_entry(vgt, head, list) {
+		if ((cur_cycles - vgt->stat.schedule_out_time) / cpu_khz >
+			GUEST_TDR_THRES_MS) {
+			if (vgt_vrings_empty(vgt) ||
+					is_current_render_owner(vgt))
+				continue;
+			else if ((cur_cycles - vgt->sched_info.last_ctx_submit_time) / cpu_khz <
+				GUEST_TDR_THRES_MS)
+				continue;
+			vgt_warn("LRU guest detect: VM-%d cur: %lld, "
+						"sched_out: %lld, "
+						"last_ctx: %lld\n",
+				vgt->vm_id, cur_cycles / cpu_khz,
+				vgt->stat.schedule_out_time / cpu_khz,
+				vgt->sched_info.last_ctx_submit_time / cpu_khz);
+			return vgt;
+		}
+	}
+
+	return NULL;
+}
+
+/* FIXME: Since it is part of "timer based scheduler",
+ * move this from vgt_context.c here and renamed from
+ * next_vgt() to tbs_next_vgt()
+ */
+static struct vgt_device *tbs_next_vgt(
+	struct list_head *head, struct vgt_device *cur_vgt)
+{
+	struct vgt_device *next_vgt = NULL;
+	struct pgt_device *pdev;
+
+	if (cur_vgt->force_removal)
+		return vgt_dom0;
+
+	pdev = cur_vgt->pdev;
+	if (ctx_switch_requested(pdev))
+		return pdev->next_sched_vgt;
+
+	next_vgt = vgt_longest_unsched(head);
+	if (next_vgt)
+		return next_vgt;
+
+	return vgt_pickup_next(head, cur_vgt);
+}
+
 /* safe to not use vgt_enter/vgt_exit, otherwise easily lead to deadlock */
 static enum hrtimer_restart vgt_tbs_timer_fn(struct hrtimer *data)
 {
-- 
1.7.10.4

